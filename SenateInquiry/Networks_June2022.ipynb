{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using networks for text analysis\n",
    "\n",
    "Graphs are a powerful tool for visualising relationships between things and networks are a kind of graph. Social scientists probably first think of networks as social networks representing relationships between people. But we can also use networks to represent relationships in texts. One quite common way of doing this is to use a network to represent collocation relationships between words - LADAL provides an excellent [tutorial](https://slcladal.github.io/coll.html#3_Visualizing_Collocations) on this topic. Here, we will use a network to visualise how words are distributed across several groups of related texts, helping us to answer the question: does the presence or absence of particular words characterise one group of texts compared to the other groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting set up\n",
    "\n",
    "We will use two packages in this session: **tm**, which is a package of text mining tools, and **igraph**, which is a graph manipulation and plotting package (also available for Python).\n",
    "\n",
    "If you do not already have these packages installed, you will need to run the code in the next cell. If you already have the packages installed, you can skip to the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install packages\n",
    "install.packages(tm)\n",
    "install.packages(igraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## activate packages\n",
    "library(tm)\n",
    "library(igraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "After loading the data file, we create five text files, one for each of the author groups represented (Federal Government, Higher Education, Individuals, Local Government and Non-Government Organisations). These five texts are then assembled as a single vector and then a **tm** corpus object is created. As usual, it's good to check that the results are what we expect (messy text in this case!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data (you will need a full pathy if this file is not in your working directory)\n",
    "senate_data <- read.csv('submission_file_categorisation_content.csv')\n",
    "\n",
    "## join text from each author group \n",
    "fed_gov_text <- paste(subset(senate_data, Category == \"FedGov\")[,7], collapse = ' ')\n",
    "hed_text <- paste(subset(senate_data, Category == \"HEd\")[,7], collapse = ' ')\n",
    "individ_text <- paste(subset(senate_data, Category == \"Individ\")[,7],collapse = ' ')\n",
    "loc_gov_text <- paste(subset(senate_data, Category == \"LocalGov\")[,7], collapse = ' ')\n",
    "ngo_text <- paste(subset(senate_data, Category == \"NGO\")[,7], collapse = ' ')\n",
    "\n",
    "## create Corpus object\n",
    "texts <- c(fed_gov_text, hed_text, individ_text, loc_gov_text, ngo_text)\n",
    "docs <- Corpus(VectorSource(texts))\n",
    "\n",
    "## inspect\n",
    "str(as.character(docs[[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "One reason we are using the **tm** package is because it makes it easy to carry out some cleaning. Most of the processes here are to make sure that we will count the various types and tokens appropriately. Warning messages may appear as these steps are executed but you don't need to worry about them. Again, we inspect the results - stemmed text looks strange the first time you see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "docs <-tm_map(docs,content_transformer(tolower))\n",
    "docs <- tm_map(docs, removePunctuation)\n",
    "docs <- tm_map(docs, removeNumbers)\n",
    "docs <- tm_map(docs, removeWords, stopwords(\"english\"))\n",
    "docs <- tm_map(docs, stripWhitespace)\n",
    "docs <- tm_map(docs, stemDocument)\n",
    "\n",
    "## inspect again\n",
    "str(as.character(docs[[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaping the data\n",
    "\n",
    "The second reason for using the **tm** package is that it has a functions to generate matrices of the occurrence of terms in documents. Here, we create a term-document matrix, but we could also create a document-term matrix which would be a transposed version of the TDM. The TDM is easier for our purposes though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a term-document matrix, make it a data frame\n",
    "TDM <- TermDocumentMatrix(docs)\n",
    "dim(TDM)\n",
    "TDM <- as.matrix(TDM)\n",
    "TDM_df <- as.data.frame(TDM)\n",
    "colnames(TDM_df) <- c('fedGov', 'HEd', 'Indiv','LocGov','NGO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in how words are distributed across submissions with different sources. For this purpose, words which appear in all five grouped documents are not interesting. Words which appear in only one grouped document may be interesting, but visualising them as a network will not be. Therefore we need to include in our data table information about how many groups of documents terms appear in. We also want information about the total number of occurrences of each term so that can order the data by frequency. Finally here, we get a count of the number of tokens in each group fo documents; later we will use this to calculate normalised frequencies of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function gets number of docs where term occurs\n",
    "in_docs <- function(rowx) {\n",
    "  number = 5\n",
    "  for (value in rowx) {\n",
    "    if (value == 0) {number <- number - 1}\n",
    "  }\n",
    "  return(number)\n",
    "}\n",
    "\n",
    "## add columns to data frame: total number of occurrences in docs, number of docs in which a term appears\n",
    "TDM_df$freq <- apply(TDM_df, sum, MARGIN = 1)\n",
    "\n",
    "in_doc_vector <- apply(TDM_df, in_docs, MARGIN = 1)\n",
    "TDM_df$inDocs <- in_doc_vector\n",
    "str(TDM_df)\n",
    "\n",
    "## get token counts for each group of docs\n",
    "token_counts <- apply(TDM_df, sum, MARGIN = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for plotting a graph\n",
    "\n",
    "The input for **igraph** to produce a graph object is a list of edges, that is, a list of pairs of nodes where each pair represents the link between two nodes. The code here is structured so that we can set three variables and easily produce plots for various combinations of them. The variables are how many documents groups the terms appear in, how many words we want to include and whether we will take words from the top or the bottom of the frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set variables for getting plot data\n",
    "## words in how many groups of documents? (2,3,4)\n",
    "doc_groups <- 3\n",
    "## number of words to include\n",
    "word_count <- 20\n",
    "## most frequent or least frequent? values 'most', 'least'\n",
    "freq <- 'most'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we extract the relevant rows from our main data table, order them and then take the relevant number of words from one end of the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get subset of data, order by frequency of terms\n",
    "plot_data <- subset(TDM_df, TDM_df$inDocs == doc_groups)\n",
    "plot_data <- plot_data[order(plot_data$freq),]\n",
    "if (freq == 'most') {\n",
    "  plot_data <- plot_data[(nrow(plot_data) - word_count):(nrow(plot_data)),]\n",
    "} else if (freq == \"least\") {\n",
    "  plot_data <- plot_data[1:word_count,]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we work through the table of relevant data and make a pair for each term and document combination. This will produce NULL entries which we prune before building the list cumulatively. We also calculate a normalised frequency of occurrence for each term, that is, the average frequency per 10,000 tokens in the (cleaned) corpus. This information will allow us to show the strength of connections in our final plot.\n",
    "Finally we make the result a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make edge table, includes edge weights (= normalised frequency)\n",
    "edge_list <- list()\n",
    "sources <- c('FedGov', 'HEd','Indiv','LocGov','NGO')\n",
    "\n",
    "for (j in 1:nrow(plot_data)) {\n",
    "  edges_y <- list()\n",
    "  data_row <- plot_data[j,]\n",
    "  for (i in 1:5) {\n",
    "    if (as.numeric(data_row[i]) != 0) {\n",
    "      edges_y[[i]] <- c(rownames(data_row), sources[i], data_row[i]/(token_counts[i]/10000))\n",
    "    }\n",
    "  }\n",
    "  edges_y = edges_y[-which(sapply(edges_y, is.null))]\n",
    "  edges_y <- do.call(rbind, edges_y)\n",
    "  edge_list <- rbind(edge_list, edges_y)\n",
    "}\n",
    "\n",
    "edge_list_df <-as.data.frame(edge_list)\n",
    "colnames(edge_list_df) <- c('term','doc','weight')\n",
    "str(edge_list_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising our results\n",
    "\n",
    "**igraph** has various functions which generate graph objects, one of them takes a data frame as input. The first two columns in the data frame must conatin the edge list.\n",
    "A bipartite graph has the property that nodes divide into two groups with edges only linking nodes from one group with nodes from the other group (i.e. there are no edges linking nodes in the same group). We check that our graph is bipartite and then assign the type labels generated by the checking function to the nodes in the graph. Attributes of the nodes and edges of the graph can be accessed via the V([graph_name]) and E([graph_name]) objects.\n",
    "Now we can take a first look at our results. If you like, you could also view the default layout **igraph** gives this data - just remove the *layout* argument from the **plot()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the graph object\n",
    "bi_plot <- graph.data.frame(edge_list_df, directed = FALSE)\n",
    "\n",
    "## check that the graph object is bipartite and assign type values to the nodes\n",
    "bipartite.mapping(bi_plot)\n",
    "V(bi_plot)$type <- bipartite_mapping(bi_plot)$type\n",
    "\n",
    "## view basic plot\n",
    "plot(bi_plot, layout = layout.bipartite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaking the plot\n",
    "\n",
    "We can add more information to our visualisation by showing the strength of connection of edges, which here means frequency of occurrence of terms. The third column of our plotting data frame has the normalised frequencies for each term in the relvant group of documents; this has become the *weight* attribute of the edges object (**E(bi_plot)$weight**). There are various ways we can use this; the two obvious ones are to scale the width of the edges or to scale the colour of the edges. Here we will scale the colour, or more precisely, the transparency of the edges.\n",
    "Transparency or *alpha* has a value between 0 (completely transparent) and 1(completely opaque). We therefore need to set a  scaling factor which takes our weight values and returns values in that range. We wnat the maximum weight value to end up close to 1, so we look at that value and then do a simple calculation and reset the weight values. To set transparency in an **igraph** plot, we have to use the **rgb()** colour setting function; This function takes four arguments: three values for the red, green and blue components on the colour (here we use straight red), and the alpha value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot with edge transparency reflecting weight\n",
    "## check max value for weight\n",
    "max(unlist((E(bi_plot)$weight)))\n",
    "\n",
    "## choose a scaling factor which will give $weight a maximummvalue close to 1\n",
    "E(bi_plot)$weight <- as.numeric(E(bi_plot)$weight) * 0.022\n",
    "\n",
    "## use $weight to set edge transparency\n",
    "plot(bi_plot, layout = layout.bipartite, edge.color = rgb(1,0,0, E(bi_plot)$weight))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the graph more readable by doing without circles around the term nodes and by making the label text smaller (so that terms overlap less). Again, we do this by setting attributes of the V([graph_name]) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## no circles for terms\n",
    "V(bi_plot)$shape <- ifelse(V(bi_plot)$type == \"TRUE\", \"circle\", \"none\")\n",
    "\n",
    "## make term labels smaller\n",
    "V(bi_plot)$label.cex <- 0.6\n",
    "\n",
    "## plot again\n",
    "plot(bi_plot, layout = layout.bipartite, edge.color = rgb(1,0,0, E(bi_plot)$weight))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 4.1.1",
   "language": "R",
   "name": "ir4.1.1"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
